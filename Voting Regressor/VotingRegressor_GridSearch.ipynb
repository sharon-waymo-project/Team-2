{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VotingRegressor_GridSearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "homLdCwFJmbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "import xgboost\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpD9raf6JvpJ",
        "colab_type": "code",
        "outputId": "430d0538-39bb-4bba-a6da-debbca964ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/RandomForest\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KTBhbR7JtG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = pd.read_csv('training_data_1_12.csv', names=[\"vx\", \"vy\", \"vz\", \"dx\", \"dy\", \"vfx\", \"vfy\", \"vfz\", \"afx\", \"afy\", \"afz\", \"num_v_labels\", \"ax\", \"ay\", \"az\"])\n",
        "test_dataset = pd.read_csv('validation_data_1_12.csv', names=[\"vx\", \"vy\", \"vz\", \"dx\", \"dy\", \"vfx\", \"vfy\", \"vfz\", \"afx\", \"afy\", \"afz\", \"num_v_labels\", \"ax\", \"ay\", \"az\"])\n",
        "train_dataset = train_dataset.drop(train_dataset.index[0])\n",
        "test_dataset = test_dataset.drop(test_dataset.index[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bx-mzuHKaf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train_dataset.iloc[:, :12]\n",
        "X_test = test_dataset.iloc[:, :12]\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_86NQyc_KuQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_ax, Y_train_ay, Y_train_az = train_dataset.iloc[:,12], train_dataset.iloc[:,13], train_dataset.iloc[:, 14]\n",
        "Y_test_ax, Y_test_ay, Y_test_az = test_dataset.iloc[:, 12], test_dataset.iloc[:, 13], test_dataset.iloc[:, 14]\n",
        "Y_train_ax, Y_train_ay, Y_train_az = Y_train_ax.astype(np.float), Y_train_ay.astype(np.float), Y_train_az.astype(np.float)\n",
        "Y_test_ax, Y_test_ay, Y_test_az = Y_test_ax.astype(np.float), Y_test_ay.astype(np.float), Y_test_az.astype(np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXasjEIfLTIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import pickle\n",
        "\n",
        "def grid_search(model_type, tuned_parameters, X_train, y_train, X_test, y_test, name):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        if name != \"VotingRegressor\":\n",
        "            clf = GridSearchCV(model_type, tuned_parameters, cv=5)\n",
        "        else:\n",
        "            clf = model_type\n",
        "        clf.fit(X_train, y_train)\n",
        "        print(\"Model is\", name)\n",
        "        print(\"\")\n",
        "        print(\"Best parameters set found on development set:\")\n",
        "        print(\"\")\n",
        "        if name != \"VotingRegressor\":\n",
        "            print(clf.best_params_)\n",
        "        print(\"\")\n",
        "\n",
        "        print(\"The model is trained on the full development set.\")\n",
        "        print(\"The scores are computed on the full evaluation set.\")\n",
        "        print(\"\")\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
        "        print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
        "        print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "        \n",
        "        return clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljDPLJgELkHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ax():\n",
        "    \n",
        "    model = RandomForestRegressor()\n",
        "\n",
        "    tuned_parameters = [{\n",
        "    'max_depth': [60, 80],\n",
        "    'max_features': [2, 3],\n",
        "    'n_estimators': [100, 200, 500]\n",
        "    }]\n",
        "\n",
        "    random_forest_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"RandomForestRegressor\")\n",
        "    \n",
        "    model = AdaBoostRegressor()\n",
        "    \n",
        "    tuned_parameters = {\n",
        "         'n_estimators': [50, 100],\n",
        "         'learning_rate' : [0.1,0.3,1],\n",
        "         'loss' : ['square', 'exponential']\n",
        "    }\n",
        "        \n",
        "    adaboost_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"AdaBoostRegressor\")\n",
        "\n",
        "    model = ExtraTreesRegressor()\n",
        "    tuned_parameters = [{\n",
        "        'n_estimators': [500], \n",
        "        'max_features': [5, 10]\n",
        "    }]\n",
        "    \n",
        "    extra_trees_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"ExtraTreesRegressor\")\n",
        "\n",
        "    model = GradientBoostingRegressor()\n",
        "    tuned_parameters = [{\n",
        "        'n_estimators':[100], \n",
        "        'learning_rate': [0.02, 0.01], \n",
        "        'max_depth':[6,4],\n",
        "        'max_features':[1.0,0.3] \n",
        "    }]\n",
        "    \n",
        "    gradient_boosting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"GradientBoostingRegressor\")\n",
        "    \n",
        "    \n",
        "    estimators = [\n",
        "     ('rfr', random_forest_regressor),\n",
        "     ('abr', adaboost_regressor),\n",
        "     ('etr', extra_trees_regressor),\n",
        "     ('gbr', gradient_boosting_regressor)\n",
        "    ]\n",
        "    \n",
        "    model = VotingRegressor(estimators)\n",
        "    \n",
        "    tuned_parameters = [{\n",
        "        \n",
        "    }]\n",
        "    \n",
        "    voting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"VotingRegressor\")\n",
        "    \n",
        "    print(\"Saving the weights of the model\")\n",
        "    pickle.dump(voting_regressor, open(\"voting_regressor_ax.sav\", 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbYalV8DL9p4",
        "colab_type": "code",
        "outputId": "de550b1d-09fc-4491-dacd-a13b6f482848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_ax()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is RandomForestRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'max_depth': 80, 'max_features': 3, 'n_estimators': 500}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.3005670583620109\n",
            "Mean Squared Error: 0.24215637656709194\n",
            "Root Mean Squared Error: 0.4920938696703018\n",
            "Model is AdaBoostRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'learning_rate': 0.1, 'loss': 'exponential', 'n_estimators': 50}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.33937997174358997\n",
            "Mean Squared Error: 0.2767001951263556\n",
            "Root Mean Squared Error: 0.5260229986667461\n",
            "Model is ExtraTreesRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'max_features': 10, 'n_estimators': 500}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.3033962245076216\n",
            "Mean Squared Error: 0.24417616606377987\n",
            "Root Mean Squared Error: 0.4941418481203346\n",
            "Model is GradientBoostingRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'learning_rate': 0.02, 'max_depth': 6, 'max_features': 1.0, 'n_estimators': 100}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.3151123834613535\n",
            "Mean Squared Error: 0.2582375654941112\n",
            "Root Mean Squared Error: 0.50817080346485\n",
            "Model is VotingRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.3056774867582203\n",
            "Mean Squared Error: 0.24381545759489054\n",
            "Root Mean Squared Error: 0.49377672848656057\n",
            "Saving the weights of the model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_mhMjoERf4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ay():\n",
        "    \n",
        "    model = RandomForestRegressor()\n",
        "\n",
        "    tuned_parameters = [{\n",
        "    'max_depth': [60, 80],\n",
        "    'max_features': [2, 3],\n",
        "    'n_estimators': [100, 200]\n",
        "    }]\n",
        "\n",
        "    random_forest_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"RandomForestRegressor\")\n",
        "    \n",
        "    model = AdaBoostRegressor()\n",
        "    \n",
        "    tuned_parameters = {\n",
        "         'n_estimators': [50, 100],\n",
        "         'learning_rate' : [0.1,0.3,1],\n",
        "         'loss' : ['square', 'exponential']\n",
        "    }\n",
        "        \n",
        "    adaboost_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"AdaBoostRegressor\")\n",
        "\n",
        "    model = ExtraTreesRegressor()\n",
        "    tuned_parameters = [{\n",
        "        'n_estimators': [500], \n",
        "        'max_features': [5, 10]\n",
        "    }]\n",
        "    \n",
        "    extra_trees_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"ExtraTreesRegressor\")\n",
        "\n",
        "    model = GradientBoostingRegressor()\n",
        "    tuned_parameters = [{\n",
        "        'n_estimators':[100], \n",
        "        'learning_rate': [0.02, 0.01], \n",
        "        'max_depth':[6,4], \n",
        "        'min_samples_leaf':[3,5,9], \n",
        "        'max_features':[1.0,0.3,0.1] \n",
        "    }]\n",
        "    \n",
        "    gradient_boosting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"GradientBoostingRegressor\")\n",
        "    \n",
        "    estimators = [\n",
        "     ('rfr', random_forest_regressor),\n",
        "     ('abr', adaboost_regressor),\n",
        "     ('etr', extra_trees_regressor),\n",
        "     ('gbr', gradient_boosting_regressor)\n",
        "    ]\n",
        "    \n",
        "    model = VotingRegressor(estimators)\n",
        "    \n",
        "    tuned_parameters = [{\n",
        "        \n",
        "    }]\n",
        "    \n",
        "    voting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"VotingRegressor\")\n",
        "    \n",
        "    print(\"Saving the weights of the model\")\n",
        "    pickle.dump(voting_regressor, open(\"voting_regressor_ay.sav\", 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2jhzHsG0jgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50e8427c-bf77-4ba0-eeb6-f80146d13a20"
      },
      "source": [
        "train_ay()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is RandomForestRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'max_depth': 80, 'max_features': 3, 'n_estimators': 100}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.251823796832479\n",
            "Mean Squared Error: 0.16807306184054044\n",
            "Root Mean Squared Error: 0.4099671472698031\n",
            "Model is AdaBoostRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'learning_rate': 0.1, 'loss': 'exponential', 'n_estimators': 50}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.2679748369615588\n",
            "Mean Squared Error: 0.1781475842283439\n",
            "Root Mean Squared Error: 0.4220753300399632\n",
            "Model is ExtraTreesRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'max_features': 10, 'n_estimators': 500}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.2775866530080545\n",
            "Mean Squared Error: 0.19515479863610258\n",
            "Root Mean Squared Error: 0.44176328348574034\n",
            "Model is GradientBoostingRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'learning_rate': 0.02, 'max_depth': 4, 'max_features': 1.0, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.2356913973525949\n",
            "Mean Squared Error: 0.1627617448524305\n",
            "Root Mean Squared Error: 0.40343741131981115\n",
            "Model is VotingRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.2464868635753453\n",
            "Mean Squared Error: 0.16010151558951097\n",
            "Root Mean Squared Error: 0.4001268743655079\n",
            "Saving the weights of the model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b07JP2ip0rVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_az():\n",
        "    \n",
        "    model = RandomForestRegressor()\n",
        "\n",
        "    tuned_parameters = [{\n",
        "    'max_depth': [60, 80],\n",
        "    'max_features': [2, 3],\n",
        "    'n_estimators': [100, 200]\n",
        "    }]\n",
        "\n",
        "    random_forest_regressor = grid_search(model, tuned_parameters, X_train, Y_train_az, X_test, Y_test_az, \"RandomForestRegressor\")\n",
        "    \n",
        "    model = AdaBoostRegressor()\n",
        "    \n",
        "    tuned_parameters = {\n",
        "         'n_estimators': [50, 100],\n",
        "         'learning_rate' : [0.1,0.3,1],\n",
        "         'loss' : ['square', 'exponential']\n",
        "    }\n",
        "        \n",
        "    adaboost_regressor = grid_search(model, tuned_parameters, X_train, Y_train_az, X_test, Y_test_az, \"AdaBoostRegressor\")\n",
        "\n",
        "    model = ExtraTreesRegressor()\n",
        "    tuned_parameters = [{\n",
        "        'n_estimators': [500], \n",
        "        'max_features': [5, 10]\n",
        "    }]\n",
        "    \n",
        "    extra_trees_regressor = grid_search(model, tuned_parameters, X_train, Y_train_az, X_test, Y_test_az, \"ExtraTreesRegressor\")\n",
        "\n",
        "    model = GradientBoostingRegressor()\n",
        "    tuned_parameters = [{\n",
        "        'n_estimators':[100], \n",
        "        'learning_rate': [0.02, 0.01], \n",
        "        'max_depth':[6,4], \n",
        "        'min_samples_leaf':[3,5,9], \n",
        "        'max_features':[1.0,0.3,0.1] \n",
        "    }]\n",
        "    \n",
        "    gradient_boosting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_az, X_test, Y_test_az, \"GradientBoostingRegressor\")\n",
        "    \n",
        "    estimators = [\n",
        "     ('rfr', random_forest_regressor),\n",
        "     ('abr', adaboost_regressor),\n",
        "     ('etr', extra_trees_regressor),\n",
        "     ('gbr', gradient_boosting_regressor)\n",
        "    ]\n",
        "    \n",
        "    model = VotingRegressor(estimators)\n",
        "    \n",
        "    tuned_parameters = [{\n",
        "        \n",
        "    }]\n",
        "    \n",
        "    voting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_az, X_test, Y_test_az, \"VotingRegressor\")\n",
        "    \n",
        "    print(\"Saving the weights of the model\")\n",
        "    pickle.dump(voting_regressor, open(\"voting_regressor_az.sav\", 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYy6fJx31FIB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b8b97d1-c4c0-468a-fadb-0098ac2b1970"
      },
      "source": [
        "train_az()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is RandomForestRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'max_depth': 80, 'max_features': 3, 'n_estimators': 100}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.10907147420254557\n",
            "Mean Squared Error: 0.038867682640267784\n",
            "Root Mean Squared Error: 0.19714888445098486\n",
            "Model is AdaBoostRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'learning_rate': 0.1, 'loss': 'exponential', 'n_estimators': 50}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.11993783749819377\n",
            "Mean Squared Error: 0.038080729281410125\n",
            "Root Mean Squared Error: 0.19514284327489473\n",
            "Model is ExtraTreesRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'max_features': 10, 'n_estimators': 500}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.10459767745746586\n",
            "Mean Squared Error: 0.03626071987721968\n",
            "Root Mean Squared Error: 0.19042247734240747\n",
            "Model is GradientBoostingRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'learning_rate': 0.02, 'max_depth': 6, 'max_features': 1.0, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.10139383200128604\n",
            "Mean Squared Error: 0.03284475823706664\n",
            "Root Mean Squared Error: 0.18123122864745644\n",
            "Model is VotingRegressor\n",
            "\n",
            "Best parameters set found on development set:\n",
            "\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Mean Absolute Error: 0.10324370944063796\n",
            "Mean Squared Error: 0.03350741640388099\n",
            "Root Mean Squared Error: 0.1830503111275176\n",
            "Saving the weights of the model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLk6426HDcVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}