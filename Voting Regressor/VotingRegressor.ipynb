{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import xgboost\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data is (17654, 15)\n",
      "Shape of the validation data is (6947, 15)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv('/home/scg2151/waymo-project/csv_data/training/training_data_1_12.csv', names=[\"vx\", \"vy\", \"vz\", \"dx\", \"dy\", \"vfx\", \"vfy\", \"vfz\", \"afx\", \"afy\", \"afz\", \"num_v_labels\", \"ax\", \"ay\", \"az\"])\n",
    "test_dataset = pd.read_csv('/home/scg2151/waymo-project/csv_data/validation/validation_data_1_12.csv', names=[\"vx\", \"vy\", \"vz\", \"dx\", \"dy\", \"vfx\", \"vfy\", \"vfz\", \"afx\", \"afy\", \"afz\", \"num_v_labels\", \"ax\", \"ay\", \"az\"])\n",
    "train_dataset = train_dataset.drop(train_dataset.index[0])\n",
    "test_dataset = test_dataset.drop(test_dataset.index[0])\n",
    "print(\"Shape of the training data is\", train_dataset.shape)\n",
    "print(\"Shape of the validation data is\", test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the X_train is (17654, 12)\n",
      "Shape of the X_test is (6947, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_dataset.iloc[:, :12]\n",
    "X_test = test_dataset.iloc[:, :12]\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "print(\"Shape of the X_train is\", X_train.shape)\n",
    "print(\"Shape of the X_test is\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Y_train_ax, Y_train_ay is (17654,) (17654,)\n",
      "Shape of the Y_test_ax, Y_test_ay is (6947,) (6947,)\n"
     ]
    }
   ],
   "source": [
    "Y_train_ax, Y_train_ay = train_dataset.iloc[:,12], train_dataset.iloc[:,13]\n",
    "Y_test_ax, Y_test_ay = test_dataset.iloc[:, 12], test_dataset.iloc[:, 13]\n",
    "Y_train_ax, Y_train_ay = Y_train_ax.astype(np.float), Y_train_ay.astype(np.float)\n",
    "Y_test_ax, Y_test_ay = Y_test_ax.astype(np.float), Y_test_ay.astype(np.float)\n",
    "print(\"Shape of the Y_train_ax, Y_train_ay is\", Y_train_ax.shape, Y_train_ay.shape)\n",
    "print(\"Shape of the Y_test_ax, Y_test_ay is\", Y_test_ax.shape, Y_test_ay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train, Y_test = train_dataset.iloc[:, 12:], test_dataset.iloc[:, 12:]\n",
    "# Y_train, Y_test = Y_train.astype(np.float), Y_test.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import pickle\n",
    "\n",
    "def grid_search(model_type, tuned_parameters, X_train, y_train, X_test, y_test, name):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if name != \"VotingRegressor\":\n",
    "            clf = GridSearchCV(model_type, tuned_parameters, cv=3)\n",
    "        else:\n",
    "            clf = model_type\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\"Model is\", name)\n",
    "        print(\"\")\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print(\"\")\n",
    "        if name != \"VotingRegressor\":\n",
    "            print(clf.best_params_)\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print(\"\")\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "        print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "        print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "\n",
    "def train_ax():\n",
    "    \n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    tuned_parameters = [{\n",
    "    'max_depth': [60, 80],\n",
    "    'max_features': [2, 3],\n",
    "    'n_estimators': [100, 200, 500]\n",
    "    }]\n",
    "\n",
    "    random_forest_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"RandomForestRegressor\")\n",
    "    \n",
    "    model = AdaBoostRegressor()\n",
    "    \n",
    "    tuned_parameters = {\n",
    "         'n_estimators': [50, 100],\n",
    "         'learning_rate' : [0.1,0.3,1],\n",
    "         'loss' : ['square', 'exponential']\n",
    "    }\n",
    "        \n",
    "    adaboost_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"AdaBoostRegressor\")\n",
    "\n",
    "    model = ExtraTreesRegressor()\n",
    "    tuned_parameters = [{\n",
    "        'n_estimators': [500], \n",
    "        'max_features': [5, 10]\n",
    "    }]\n",
    "    \n",
    "    extra_trees_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"ExtraTreesRegressor\")\n",
    "\n",
    "    model = GradientBoostingRegressor()\n",
    "    tuned_parameters = [{\n",
    "        'n_estimators':[100], \n",
    "        'learning_rate': [0.02, 0.01], \n",
    "        'max_depth':[6,4], \n",
    "        'min_samples_leaf':[3,5,9], \n",
    "        'max_features':[1.0,0.3,0.1] \n",
    "    }]\n",
    "    \n",
    "    gradient_boosting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"GradientBoostingRegressor\")\n",
    "    \n",
    "    \n",
    "    estimators = [\n",
    "     ('rfr', random_forest_regressor),\n",
    "     ('abr', adaboost_regressor),\n",
    "     ('etr', extra_trees_regressor),\n",
    "     ('gbr', gradient_boosting_regressor)\n",
    "    ]\n",
    "    \n",
    "    model = VotingRegressor(estimators)\n",
    "    \n",
    "    tuned_parameters = [{\n",
    "        \n",
    "    }]\n",
    "    \n",
    "    voting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ax, X_test, Y_test_ax, \"VotingRegressor\")\n",
    "    \n",
    "    print(\"Saving the weights of the model\")\n",
    "    pickle.dump(voting_regressor, open(\"voting_regressor_ax.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is RandomForestRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 60, 'max_features': 3, 'n_estimators': 500}\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.3006799610771246\n",
      "Mean Squared Error: 0.24145193599545425\n",
      "Root Mean Squared Error: 0.4913775900419699\n",
      "Model is AdaBoostRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_estimators': 50, 'learning_rate': 0.1, 'loss': 'exponential'}\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.33847888604625914\n",
      "Mean Squared Error: 0.27571302999741376\n",
      "Root Mean Squared Error: 0.525083831399724\n",
      "Model is ExtraTreesRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_estimators': 500, 'max_features': 10}\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.30811764451818585\n",
      "Mean Squared Error: 0.24995978300802793\n",
      "Root Mean Squared Error: 0.49995978139049135\n",
      "Model is GradientBoostingRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 6, 'learning_rate': 0.02, 'min_samples_leaf': 5, 'max_features': 1.0, 'n_estimators': 100}\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.311895781579496\n",
      "Mean Squared Error: 0.2530942921730238\n",
      "Root Mean Squared Error: 0.5030847763280298\n",
      "Model is VotingRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.3044532541296203\n",
      "Mean Squared Error: 0.24199713044447133\n",
      "Root Mean Squared Error: 0.4919320384407498\n",
      "Saving the weights of the model\n"
     ]
    }
   ],
   "source": [
    "train_ax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "def train_ay():\n",
    "    \n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    tuned_parameters = [{\n",
    "    'max_depth': [60, 80],\n",
    "    'max_features': [2, 3],\n",
    "    'n_estimators': [100, 200]\n",
    "    }]\n",
    "\n",
    "    random_forest_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"RandomForestRegressor\")\n",
    "    \n",
    "    model = AdaBoostRegressor()\n",
    "    \n",
    "    tuned_parameters = {\n",
    "         'n_estimators': [50, 100],\n",
    "         'learning_rate' : [0.1,0.3,1],\n",
    "         'loss' : ['square', 'exponential']\n",
    "    }\n",
    "        \n",
    "    adaboost_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"AdaBoostRegressor\")\n",
    "\n",
    "    model = ExtraTreesRegressor()\n",
    "    tuned_parameters = [{\n",
    "        'n_estimators': [500], \n",
    "        'max_features': [5, 10]\n",
    "    }]\n",
    "    \n",
    "    extra_trees_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"ExtraTreesRegressor\")\n",
    "\n",
    "    model = GradientBoostingRegressor()\n",
    "    tuned_parameters = [{\n",
    "        'n_estimators':[100], \n",
    "        'learning_rate': [0.02, 0.01], \n",
    "        'max_depth':[6,4], \n",
    "        'min_samples_leaf':[3,5,9], \n",
    "        'max_features':[1.0,0.3,0.1] \n",
    "    }]\n",
    "    \n",
    "    gradient_boosting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"GradientBoostingRegressor\")\n",
    "    \n",
    "    estimators = [\n",
    "     ('rfr', random_forest_regressor),\n",
    "     ('abr', adaboost_regressor),\n",
    "     ('etr', extra_trees_regressor),\n",
    "     ('gbr', gradient_boosting_regressor)\n",
    "    ]\n",
    "    \n",
    "    model = VotingRegressor(estimators)\n",
    "    \n",
    "    tuned_parameters = [{\n",
    "        \n",
    "    }]\n",
    "    \n",
    "    voting_regressor = grid_search(model, tuned_parameters, X_train, Y_train_ay, X_test, Y_test_ay, \"VotingRegressor\")\n",
    "    \n",
    "    print(\"Saving the weights of the model\")\n",
    "    pickle.dump(voting_regressor, open(\"voting_regressor_ay.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is RandomForestRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 60, 'max_features': 3, 'n_estimators': 200}\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.25060053238122937\n",
      "Mean Squared Error: 0.16930031741179055\n",
      "Root Mean Squared Error: 0.41146119794190866\n",
      "Model is AdaBoostRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_estimators': 100, 'learning_rate': 0.1, 'loss': 'exponential'}\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.2879121633963699\n",
      "Mean Squared Error: 0.18929846315048285\n",
      "Root Mean Squared Error: 0.4350844322088333\n",
      "Model is ExtraTreesRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_estimators': 500, 'max_features': 10}\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.2778544675184178\n",
      "Mean Squared Error: 0.19537327288030484\n",
      "Root Mean Squared Error: 0.44201048955913347\n",
      "Model is GradientBoostingRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 6, 'learning_rate': 0.02, 'min_samples_leaf': 3, 'max_features': 1.0, 'n_estimators': 100}\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.2378741169280465\n",
      "Mean Squared Error: 0.16406813272033038\n",
      "Root Mean Squared Error: 0.4050532467717428\n",
      "Model is VotingRegressor\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "Mean Absolute Error: 0.24476022342837545\n",
      "Mean Squared Error: 0.1584848976581654\n",
      "Root Mean Squared Error: 0.39810161725138143\n",
      "Saving the weights of the model\n"
     ]
    }
   ],
   "source": [
    "train_ay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
